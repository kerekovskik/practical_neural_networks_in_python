{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cost Functions\n",
        "\n",
        "In this lab, we'll be exploring cost functions, which are a pivotal part of how machine learning algorithms, particularly neural networks, learn from data. Just like a GPS navigation system provides feedback to guide a driver towards their destination, cost functions help guide the machine learning model towards the best solution in the vast 'terrain' of parameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is a Cost Function?\n",
        "\n",
        "A cost function (also referred to as 'loss function' or 'objective function') is a measure of how 'off' a machine learning model's predictions are from the actual outcomes. The objective of the training process is to minimize this difference. This is achieved through an iterative process of adjusting the model's internal parameters, guided by the gradient of the cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of Cost Functions\n",
        "\n",
        "There are various types of cost functions used in machine learning, each with their specific uses and characteristics. We'll go over some of the most commonly used cost functions: Mean Squared Error (MSE), Cross-Entropy, and Log Loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Squared Error (MSE)\n",
        "\n",
        "MSE is commonly used in regression problems. It calculates the square of the difference between the actual and predicted values, giving a quadratic penalty to larger errors.\n",
        "\n",
        "Mathematically, for N total instances and where y is the actual value and y_hat is the predicted value, it is defined as:\n",
        "\n",
        "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def mse(y, y_hat):\n",
        "    \"\"\"\n",
        "    This function computes the mean squared error (MSE).\n",
        "\n",
        "    Args:\n",
        "    y : numpy array, true values\n",
        "    y_hat : numpy array, predicted values\n",
        "    \n",
        "    Returns:\n",
        "    float, Mean Squared Error\n",
        "    \"\"\"\n",
        "    return ((y - y_hat) ** 2).mean()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Entropy\n",
        "\n",
        "Cross-entropy is primarily used for binary classification problems, though it can be used for multi-class classification problems as well. It measures the dissimilarity between the true label distribution and the predicted label distribution.\n",
        "\n",
        "For binary classification, where y is the actual class (0 or 1) and p(y) is the predicted probability of the instance being in class 1, the formula is:\n",
        "\n",
        "$$Cross-Entropy = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i\\log(p(y_i)) + (1-y_i)\\log(1-p(y_i))]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cross_entropy(y, p):\n",
        "    \"\"\"\n",
        "    This function computes the cross-entropy.\n",
        "\n",
        "    Args:\n",
        "    y : numpy array, true values\n",
        "    p : numpy array, predicted probabilities\n",
        "    \n",
        "    Returns:\n",
        "    float, Cross-Entropy\n",
        "    \"\"\"\n",
        "    return -(y * np.log(p) + (1 - y) * np.log(1 - p)).mean()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log Loss\n",
        "\n",
        "Log loss is a slight variation of the cross-entropy function and is also used for classification problems. Log loss penalizes both types of errors (false positives and false negatives), but it gives much more weight to outputs that are far off from the actual class.\n",
        "\n",
        "For binary classification, it is identical to the formula for cross-entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing the Right Cost Function\n",
        "\n",
        "The choice of cost function can depend on the problem at hand. Here are some guidelines:\n",
        "\n",
        "- **Regression problems:** Mean Squared Error or Mean Absolute Error are commonly used.\n",
        "- **Binary classification problems:** Cross-Entropy or Log Loss are often used.\n",
        "- **Multi-class classification problems:** Multi-class Cross-Entropy is a good option.\n",
        "\n",
        "However, these are not hard and fast rules. The best cost function also depends on the specific data and task. For example, if outliers are a concern in a regression problem, you may opt for Mean Absolute Error (which is less sensitive to outliers) instead of Mean Squared Error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Examples\n",
        "\n",
        "Choosing the right cost function can have a significant impact on the performance of the model. Here are some examples of scenarios where different cost functions would be appropriate:\n",
        "\n",
        "- **Predicting house prices (a regression problem):** Here, we might use the Mean Squared Error. If the model makes a prediction that's off by $100,000, the squared penalty would incentivize the model to correct large errors.\n",
        "\n",
        "- **Email spam detection (a binary classification problem):** Cross-Entropy or Log Loss would be suitable here, as these functions work well with problems where the output can be one of two possible classes (spam or not spam).\n",
        "\n",
        "- **Digit recognition (a multi-class classification problem):** Multi-class Cross-Entropy could be used here, as we are dealing with multiple classes (ten digits from 0 to 9)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Cost functions play an essential role in the learning process of neural networks and other machine learning models. Understanding different cost functions and their implications can help make informed decisions when designing your machine learning models. It's crucial to match the cost function with the problem type, characteristics of the data, and the specific goals of your model."
      ]
    }
  ]
}
