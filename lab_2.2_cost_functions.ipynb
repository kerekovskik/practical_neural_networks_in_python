{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions\n",
    "A cost function, also known as a loss function or an error function, plays a vital role in machine learning, particularly in training neural networks. It measures the difference between the predicted output and the actual output of the model â€” essentially providing feedback on how 'far off' the model's predictions are from the actual outcomes.\n",
    "\n",
    "In the context of a GPS navigation system, think of the cost function as a measure of 'distance' between the current location (current prediction) and the destination (correct outcome). The cost function helps the model navigate through the 'terrain' of parameter space, adjusting the parameters of the model in order to minimize this 'distance' and arrive at the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)\n",
    "MSE is typically used for regression problems. It calculates the average of the squares of the differences between the predicted and actual values. Mathematically, it's represented as:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the total number of data points\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y_i}$ is the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error.\"\"\"\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse\n",
    "\n",
    "# Example usage\n",
    "y_true = np.array([1.0, 1.5, 2.0, 2.5, 3.0])\n",
    "y_pred = np.array([0.8, 1.5, 1.8, 2.6, 3.2])\n",
    "\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "Cross-Entropy is often used for binary or multi-class classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "For binary classification, it's calculated as:\n",
    "\n",
    "$$Cross-Entropy = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})]$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the total number of data points\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y_i}$ is the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Calculate Binary Cross-Entropy Loss.\"\"\"\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce\n",
    "\n",
    "# Example usage\n",
    "y_true = np.array([0, 0, 1, 1, 0])\n",
    "y_pred = np.array([0.1, 0.2, 0.9, 0.8, 0.1])\n",
    "\n",
    "print(f'Binary Cross-Entropy Loss: {binary_cross_entropy(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss\n",
    "Log Loss is essentially the same as cross-entropy for binary classification. It quantifies the accuracy of a classifier by penalising false classifications. It's calculated as:\n",
    "\n",
    "$$Log Loss = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})]$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the total number of data points\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y_i}$ is the predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right cost function\n",
    "The choice of cost function can depend on the problem at hand:\n",
    "\n",
    "- **Regression Problems:** Mean Squared Error (MSE) is a common choice as it penalises larger errors more due to squaring. Other options include Mean Absolute Error (MAE) and Huber loss.\n",
    "\n",
    "- **Binary Classification:** Binary Cross-Entropy/Log Loss is often used. It penalises predictions that are confident and wrong.\n",
    "\n",
    "- **Multi-class Classification:** Multi-class Cross-Entropy is a common choice. Similar to binary cross-entropy, but generalised to multiple classes.\n",
    "\n",
    "It's worth noting that the choice of loss function is not always straightforward and it can be beneficial to experiment with different loss functions for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world examples of cost function usage\n",
    "Different cost functions can be appropriate for different scenarios. Here are a few examples:\n",
    "\n",
    "- **Mean Squared Error (MSE):** A real estate company trying to predict house prices based on features like location, size, and condition of the house. In this case, it's a regression problem and using MSE as the cost function would be suitable.\n",
    "\n",
    "- **Cross-Entropy:** A healthcare company building a model to predict whether a patient has a disease (yes/no) based on their symptoms. This is a binary classification problem and using binary cross-entropy as the cost function would be appropriate.\n",
    "\n",
    "- **Log Loss:** A tech company building a spam email detector. The model classifies emails into 'spam' or 'not spam' categories. This is a binary classification problem and using log loss as the cost function could work well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
