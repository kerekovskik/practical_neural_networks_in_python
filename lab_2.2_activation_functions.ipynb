{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Activation Functions\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the role of activation functions in a neural network\n",
    "- Compare different types of activation functions\n",
    "- Understand the pros and cons of each function\n",
    "- Understand real-world examples where different activation functions might be used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Activation Functions\n",
    "\n",
    "An activation function in a neural network is a mathematical 'gate' in between the input feeding the current neuron and its output going to the next layer. It can be thought of as a decision-making process, determining whether a neuron should be activated or not, much like how a manager decides whether to greenlight a project based on certain criteria. This decision is made based on the weighted sum of the input to the neuron. If this weighted sum is above a certain value (the neuron's threshold), the neuron is activated and sends its own signal onward. If not, it remains inactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of Activation Functions\n",
    "\n",
    "There are several types of activation functions used in neural networks, each with their own advantages and disadvantages. The most common activation functions are:\n",
    "\n",
    "- Linear or Identity function\n",
    "- Sigmoid function\n",
    "- Hyperbolic Tangent (tanh) function\n",
    "- Rectified Linear Unit (ReLU) function\n",
    "- Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the functions\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Visualize the functions\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(x, linear(x), label='Linear')\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid')\n",
    "plt.plot(x, tanh(x), label='Tanh')\n",
    "plt.plot(x, relu(x), label='ReLU')\n",
    "plt.title('Activation Functions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Activation Functions\n",
    "\n",
    "Each activation function has its pros and cons, and are used in different scenarios based on these properties:\n",
    "\n",
    "- **Linear or Identity function**: The linear function is a straight line which means the output will be directly proportional to the input. However, using a linear activation function means that the derivative is constant, and so the gradient has no relationship with X. It's usually used in the output layer for regression problems.\n",
    "\n",
    "- **Sigmoid function**: The sigmoid function is smooth and bounded between 0 and 1. This nice property allows us to interpret the outputs as probabilities. However, it has two main disadvantages. First, its output isnâ€™t zero-centered which can make the gradient updates go too far in different directions. Second, it suffers from the vanishing gradients problem.\n",
    "\n",
    "- **Hyperbolic Tangent function (tanh)**: The tanh function is a scaled version of the sigmoid function, and its output is zero-centered because its range is -1 to 1. However, it also suffers from the vanishing gradients problem.\n",
    "\n",
    "- **Rectified Linear Unit function (ReLU)**: The ReLU function is the most widely used activation function in the field of deep learning. It's computationally efficient and helps mitigate the vanishing gradient problem. However, it suffers from the dying ReLU problem where a large gradient update can cause the neuron to stop learning entirely.\n",
    "\n",
    "- **Softmax function**: The Softmax function is generally used in the output layer for multi-class classification problems. It gives the probability distribution for each class, but it also suffers from the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world Examples\n",
    "\n",
    "Different activation functions are used in various types of neural networks, and the choice depends on the network's architecture and the problem that's being solved:\n",
    "\n",
    "- The **linear function** is often used in the output layer of a regression neural network where the output is a real value, such as predicting house prices.\n",
    "\n",
    "- The **sigmoid function** is often used in the output layer of a binary classification neural network where the output is a probability that the input point belongs to one class or the other.\n",
    "\n",
    "- The **tanh function** can be used in the hidden layers of a neural network as it is zero-centered, leading to easier model training.\n",
    "\n",
    "- The **ReLU function** is often used in the hidden layers of a deep neural network due to its computational efficiency and its ability to propagate gradients well.\n",
    "\n",
    "- The **softmax function** is used in the output layer of a multi-class classification neural network where the outputs are probabilities for each class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
