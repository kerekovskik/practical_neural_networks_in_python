{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Neurons and the Architecture of Neural Networks"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will explore how neurons are organized into layers within a neural network and how these layers interact to process data and learn from it. We will also delve into the concepts of depth and width of a neural network, and how they affect a network's learning capability."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing a Simple Neuron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ],
      "source": [
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define a single neuron\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Calculate the net input for this neuron\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1])  # example weights\n",
        "bias = 4   # example bias\n",
        "\n",
        "# Create a neuron\n",
        "neuron = Neuron(weights, bias)\n",
        "\n",
        "inputs = np.array([2, 3])  # example inputs\n",
        "print(neuron.forward(inputs))  # expected output: 0.9990889488055994"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing a Simple Neural Network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, you can implement a simple neural network in Python using NumPy. Organize neurons into layers and implement forward propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights with random values\n",
        "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
        "                                       (self.hidden_nodes, self.input_nodes))\n",
        "        \n",
        "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
        "                                       (self.output_nodes, self.hidden_nodes))\n",
        "        \n",
        "        # Define the sigmoid activation function\n",
        "        self.sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        # Forward pass through the network\n",
        "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)\n",
        "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
        "\n",
        "        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)\n",
        "        final_outputs = final_inputs  # For a regression task, the activation function can be f(x)=x \n",
        "        \n",
        "        return final_outputs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Propagation with a Given Input"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, perform forward propagation on a given input. You can use the neural network you implemented above."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Forward propagation is the initial step in training a neural network. It is the process by which we pass our input data through the network to get an output. This output can then be compared with the actual value to compute the error. This error is then used in the subsequent step, backpropagation, to adjust the weights and biases of the network.\n",
        "\n",
        "Let's break down the steps involved in forward propagation:\n",
        "\n",
        "1. **Input Layer**: The process begins with feeding the input data into the network. In our example, the input data is a numpy array of size equal to the number of input nodes. \n",
        "\n",
        "2. **Hidden Layer(s)**: Each input is multiplied by a weight, and the results are summed up, then passed through an activation function (sigmoid, in our case) to determine the output of the neuron. This is done for each neuron in the hidden layer.\n",
        "\n",
        "3. **Output Layer**: The outputs from the hidden layer neurons are again multiplied by weights and summed up. They are then passed through an activation function to determine the final output of the network.\n",
        "\n",
        "## Expected Result\n",
        "\n",
        "In this simple neural network, the expected result after forward propagation is a single scalar value, which is the output of the network given the input data. This output value is the network's prediction based on its current state (i.e., its current weights).\n",
        "\n",
        "Please note that because we're initializing our weights randomly, the initial output of the network will likely be far from the actual value (if we have one). The goal of training the network is to adjust the weights and biases such that the output of the network gets closer to the actual values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.0925791]\n"
          ]
        }
      ],
      "source": [
        "# Create a Neural Network with 3 input nodes, 4 hidden nodes, and 1 output node\n",
        "nn = NeuralNetwork(3, 4, 1)\n",
        "\n",
        "# Example input\n",
        "input_data = np.array([0.5, -0.2, 0.1])\n",
        "\n",
        "print(nn.forward_pass(input_data))  # Outputs the result of the forward pass\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
